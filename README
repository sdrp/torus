README
Torus Checkers
Sudheesha Perera
April 2016

INTRO
-----
The directory contains two Python scripts: "torus.py" and "play_torus". The first executes a single move given an input board (mostly for spot-checking the AI algorithms), and the second simulates entire games between two AI opponents. Both can be run from the command line as executables or using the Python interpreter. To jump in without wasting time with this README, download the repo and run "> ./play_torus" from the command line. The script is interactive and will walk you through the full functionality of the program. 

BACKGROUND
----------
Torus checkers is a variant of the standard boardgame checkers. The main differentiating factor is from the pieces' perspective, the regular 8x8 board is connected in a torus. That is, its top and bottom are connects and its left and right sides are connect. Pieces that move off the top of the board re-appear at the bottom of the board, and pieces that move off of the left side of the board re-appear on the right side. Consequently there are no "kings" in torus checkers. The torus structure of the board makes for interesting winning strategies, and is a great model for investigating AI algorithms. Compared to regular checkers, there are vastly more possible moves from most board states and the strategy of play is less intuitive to people who have experience with standard checkers. 


THE BOARD FILE FORMAT
---------------------

Consider the board file format below:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
g 8 8 100
p 0 0
r 12 1 2 3 4 5 6 7 8 9 10 11 12
w 12 21 22 23 24 25 26 27 28 29 30 31 32
An example game file for the initial position on a standard board. No comment character is required because all input after the fourth line is ignored.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The format above stores all of the information needed to understand the state of a game at any given moment. This specific file encodes the start of a new game of torus checkers - it refers to the first move to be played by red on a standard 8x8 board with a turn limit of 100 (a draw is reached when the turn limit is exceeded). The pieces are laid out in their standard starting positions. For reference, this board file is located in the a file named "start_board_file". This is the file used to start every game in "play_torus".

The output of "torus.py" is a single line containing first the letter "m", then the turn number, followed by the a 0/1 depending on whether red or white respectively are to move this turn. All subsequent integers represent the squares that the chosen piece touches from where it started until the end of the move. For a simple move or single capture we expect just two integers then (the original location of the piece and the square it was moved to). An example is below:
~~~~~~~~~~~
m 0 0 12 13
~~~~~~~~~~~
Here we have the first move of the game made by red, which moved its piece on square 12 diagonally off the right side of the board to square 13 on the left side of the board. This move highlights the way in which the board is connected in a torus. 

The way the script is designed, the output line above will be followed by a description of which player was to move, what string of squares were in included in the move, and ASCII printouts of the board before and after the move. For example:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
m 0 0 9 14
Player to Move: |RED|
The Move: [9, 14]
Board After Move
[_][R][_][R][_][R][_][R]
[R][_][R][_][R][_][R][_]
[_]{1}[_][R][_][R][_][R]
[_][_]{R}[_][_][_][_][_]
[_][_][_][_][_][_][_][_]
[W][_][W][_][W][_][W][_]
[_][W][_][W][_][W][_][W]
[W][_][W][_][W][_][W][_]
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The {1} represents the square that the chosen red piece started on (ie. 9) and {R} represents the new location of the piece (ie. 14). In multi-capture moves, the previous locations of the chosen piece are number in order as {1}, {2}, {3}, etc. so that the direction of the move is easier to pick out. 



HOW TO RUN: play_torus
---------------------------
From the command line, simply excecute the following:

> ./play_torus

Once execution begins, you will be prompted to choose the parameters of the game. This is an interactive script that will pit two AI algorithms against each other to see which is superior at playing torus checkers. All of the AI algorithms use an alpha-beta search to recursively determine the best move given the state of the board (alpha-beta search is an improvement on the more basic mini-max algorithm for two-player perfect information games). This script allows you to set the static evaluation function and recursive depth for each AI players. Note that while increasing the recursive depth will improve play to certain extent, it also dramatically increases the delay in returning the optimal move when the depth is set to a value greater than 4. 

HOW TO RUN: torus.py
---------------------------------  
From the command line, excecute the following:

> python torus.py path_to_board_file

Here the only command line argument is the path to a board file in the format given above. Based on the state of the game encoded in the input board file, the program will execute the optimal move for the play whose turn it is (either red or white) and output the results to the console. Note that this script will by default use a recursive depth of 3 and a basic static evaluation function based on the number of pieces of each color left on the board. To experiment with other AI parameters, use the "play_torus" script instead. The intention in maintaining the two scripts is to separate the logic common to all of the AI algorithms from the specific static evaluation functions being compared and the implementation of having two AIs play each other. The "torus.py" file also acts as a convenient stop-gap to test for errors in implementation and spot-check the behavior of the program on specific boards of interest. 


VERIFICATION AND ANALYIS OF METHOD
----------------------------------

Step 1) Basic Static Evaluation Function (A Sanity Check)
￼￼￼
First I tested the most straightforward and simple static evaluation function that came to mind - a function that returns the difference between the number of red pieces on the board and the number of white pieces on the board. When both red and white play based on this function (named "basic()" in my code) the red player always wins. This is entirely predictable, since red has the advantage of the first move and from then on the game proceeds identically during every run. Additionally, when one player is offered a greater search depth than the other (ex. white is allowed to search to depth 3, but red is only allowed to search to depth 2) the player with greater search depth always wins. This, too, is entirely predictable, since greater search depth allows for a smarter decision. For a better sanity check I implemented an entirely random static evaluation function (called "fully_random()" in my code) that arbitrarily returns a value [0, 1] without taking the placement of pieces into account. As would be expected, the player utilizing "fully_random()" always loses to the player utilizing "basic()". That is, best-of-five matches ended in either {RED: 5, WHITE: 0, DRAW: 0} or {RED: 0, WHITE: 5, DRAW: 0}. This validated that the simple static evaluation function "basic()" was at least smarter than random play.

Step 2) Basic + Random Static Evluation Function
Next I created a static evaluation function "basic_plus_random(p)" that makes a random evaluation of the input board with probability p and utilizes the "basic()" evaluation function otherwise. When p is 0.1 or greater I found that the player utilizing "basic_plus_random(p)" would almost always lose to the player who stuck with the "basic()" function. However, with very low values of p I got some interesting results. For example, when I pitted red using "basic_plus_random(0.01)" against white using "basic()" the result of a best-of-15 match was {RED: 6, WHITE: 3, DRAW: 6}. In other words, RED will draw and even lose some matches if it makes a random evaluation just 1 out of 100 times. This finding was not profound, but at least mildly interesting because for the first time the outcomes of an individual game was not entirely predictable.

Step 3) Weighted Function
My original conception for a weighted static evaluating function involved assigning each position on the board a weight when calculating the overall score. The weighting function would value positions in the center of the board more than those on the periphery, with the concept that pieces in the center of the board are more instrumental in planning a long-term winning strategy. However, I found that this weighting scheme (in the code as weighted()) failed to beat even the basic() static evaluation function. I soon realized that the basic() function had the right idea: that the raw difference in the number of pieces that each player had left really did mean more than the position of the pieces themselves. Only when both players had the same number of pieces was it necessary to use positional weighting to break the tie. Thus I created the function weighted_refined(), which calculates the weighted board score as in the weighted() function, but then scales the weighted value to between 0 and 1 before adding it to the output of the basic() function. This means that the weighting will never exceed the value of a single piece, and will only make a difference in the case of a tie.

Step 4) Testing the Weighted Function
Now the question was whether I could figure out the set of weights that would result in optimal play. The weighted_refined function takes in a list of 4 floats, the first being the weight of the center two squares, the second being the weight of the surrounding inner right, the third being the weight of the outer ring, and the last being the weight of the outermost loop around the board. My intuition was that the weighting function should most heavily consider pieces at the center of the board, and give less value to those on the periphery. However, to my surprise the weighted input [4, 3, 2, 1] performed no better than the basic static evaluation function. Since my intuition was clearly off, I generated all weighted lists containing 0 and 1 (all 0/1 strings of length 4). This gave me 16 different versions of the weighted_refined function to play with.

Here are my results:
[0, 1, 0, 0].........Number of Wins = 6 
[0, 1, 0, 1].........Number of Wins = 0 
[0, 0, 1, 0].........Number of Wins = 8 
[0, 0, 1, 1].........Number of Wins = 6 
[1, 0, 1, 1].........Number of Wins = 4 
[1, 0, 1, 0].........Number of Wins = 5 
[1, 1, 1, 0].........Number of Wins = 2 
[1, 1, 1, 1].........Number of Wins = 2 
[0, 0, 0, 1].........Number of Wins = 4 
[0, 0, 0, 0].........Number of Wins = 0 
[0, 1, 1, 1].........Number of Wins = 6 
[0, 1, 1, 0].........Number of Wins = 5 
[1, 1, 0, 0].........Number of Wins = 4 
[1, 0, 0, 0].........Number of Wins = 6 
[1, 0, 0, 1].........Number of Wins = 4 
[1, 1, 0, 1].........Number of Wins = 0

Keeping in mind that the maximum number of wins for any given set was 15 and that there were some draws, it is clear that no one set of weights was dominant. However, we find that the sets [1, 1, 1, 1] and [0, 0, 0, 0] performed quite poorly. In fact, the former failed to win a single game. This is encouraging because both of these weight sets essentially reduce the weighted_refined function to the basic function by providing uniformly proportional weights to all positions. From these results it appears that the weighting [0, 0, 1, 0] is the optimal 0/1 weighting scheme. If I were to re-visit the project, I would investigate more complicated weight distributions, involving non-binary weights and a more fine-grain breakdown of the locations on the board. Other location-weighting schemes could involve weightings for different diagonals on the board or different weightings for pieces that are stacked diagonally to avoid capture in the upcoming turn. 


